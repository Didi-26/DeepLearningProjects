{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **NEW (future train.py)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import dlc_practical_prologue as prologue\n",
    "import numpy as np\n",
    "# Our modules\n",
    "import models # contains all our torch models classes\n",
    "import plots # custom ploting functions to produce figures of the report\n",
    "import training_functions # all our functions and classes for training\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Set here which experiment is to be run (running them all will take a long time to compute)\n",
    "run_LossCompare = False\n",
    "# Experiments for \"PairSetup\" : In this setup, we consider directly the pairs as input to the \n",
    "# network. Thus our inputs are N samples of [2,14,14] made of two 14*14 pictures. Our ouputs are\n",
    "# the class 0 or 1  indicating whereas if the first digit is lesser or equal to the second.\n",
    "run_PairSetup_SimpleLinear = False\n",
    "run_PairSetup_MLP = True\n",
    "# Experiments for \"AuxiliarySetup\" : In this setup, we consider N individual 14*14 pictures as \n",
    "# input. The network use an auxiliary loss to learn to classify those from 0 to 9. The auxiliary\n",
    "# outputs are the the class 0 to 9 corresponding to the digit on the picture. We then use this \n",
    "# network to predict the number and we can then do the difference to perform our original goal\n",
    "# which is to predict whereas if the first digit is lesser or equal to the second\n",
    "run_AuxiliarySetup_SimpleLinear = False\n",
    "run_AuxiliarySetup_MLP = False\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "\n",
    "if run_LossCompare :\n",
    "    torch.manual_seed(random_seed)\n",
    "    print('******************** Running Loss comparaison ********************')\n",
    "    \n",
    "    print('--- PairSetup, MSE loss : ---')\n",
    "    in_dim, out_dim = 14*14*2, 2\n",
    "    model = models.MLP(4, 32, in_dim, out_dim)\n",
    "    test_err_mean, test_err_std, _, _ = training_functions.rounds_train(model, \n",
    "                                                                        'PairSetup',\n",
    "                                                                        #plot_title = 'Test',\n",
    "                                                                        #plot_file_path='./plots/test1.svg',\n",
    "                                                                        lr = 0.00008,\n",
    "                                                                        epochs = 200,\n",
    "                                                                        use_crossentropy=False)\n",
    "    print('mean minimum test error : {0:.{1}f} %'.format(test_err_mean,1))\n",
    "    print('std minimum test error : {0:.{1}f} %'.format(test_err_std,1))\n",
    "\n",
    "    print('--- PairSetup, cross-entropy loss : ---')\n",
    "    model = models.MLP(4, 32, in_dim, out_dim)\n",
    "    test_err_mean, test_err_std, _, _ = training_functions.rounds_train(model, \n",
    "                                                                        'PairSetup',\n",
    "                                                                        #plot_title = 'Test',\n",
    "                                                                        #plot_file_path='./plots/test2.svg',\n",
    "                                                                        lr = 0.00008,\n",
    "                                                                        epochs = 200,\n",
    "                                                                        use_crossentropy=True)\n",
    "    print('mean minimum test error : {0:.{1}f} %'.format(test_err_mean,1))\n",
    "    print('std minimum test error : {0:.{1}f} %'.format(test_err_std,1))\n",
    "    print('--- AuxiliarySetup, MSE loss : ---')\n",
    "    in_dim, out_dim = 14*14, 10\n",
    "    model = models.MLP(4, 32, in_dim, out_dim)\n",
    "    test_err_mean, test_err_std, _, _ = training_functions.rounds_train(model, \n",
    "                                                                        'AuxiliarySetup',\n",
    "                                                                        #plot_title = 'Test',\n",
    "                                                                        #plot_file_path='./plots/test1Aux.svg',\n",
    "                                                                        lr = 0.00008,\n",
    "                                                                        epochs = 200,\n",
    "                                                                        use_crossentropy=False)\n",
    "    print('mean minimum test error : {0:.{1}f} %'.format(test_err_mean,1))\n",
    "    print('std minimum test error : {0:.{1}f} %'.format(test_err_std,1))\n",
    "    \n",
    "    print('--- AuxiliarySetup, cross-entropy loss : ---')\n",
    "    model = models.MLP(4, 32, in_dim, out_dim)\n",
    "    test_err_mean, test_err_std, _, _ = training_functions.rounds_train(model,\n",
    "                                                                        'AuxiliarySetup',\n",
    "                                                                        #plot_title = 'Test',\n",
    "                                                                        #plot_file_path='./plots/test2Aux.svg',\n",
    "                                                                        lr = 0.00008,\n",
    "                                                                        epochs = 200,\n",
    "                                                                        use_crossentropy=True)\n",
    "    print('mean minimum test error : {0:.{1}f} %'.format(test_err_mean,1))\n",
    "    print('std minimum test error : {0:.{1}f} %'.format(test_err_std,1))\n",
    "    \n",
    "\n",
    "if run_PairSetup_SimpleLinear :\n",
    "    torch.manual_seed(random_seed)\n",
    "    print('******************** Running SimpleLinear model (for PairSetup) ********************')\n",
    "    in_dim, out_dim = 14*14*2, 2\n",
    "    model = models.SimpleLinear(in_dim, out_dim)\n",
    "    test_err_mean, test_err_std, _, _ = training_functions.rounds_train(model,\n",
    "                                                                        'PairSetup',\n",
    "                                                                        #plot_title = 'Linear (PairSetup) Train & Test errors',\n",
    "                                                                        #plot_file_path='./plots/pairSetup_SimpleLinear.svg',\n",
    "                                                                        lr = 0.00005,\n",
    "                                                                        epochs = 200,\n",
    "                                                                        use_crossentropy=False)\n",
    "    print('mean minimum test error : {0:.{1}f} %'.format(test_err_mean,1))\n",
    "    print('std minimum test error : {0:.{1}f} %'.format(test_err_std,1))\n",
    "\n",
    "\n",
    "if run_PairSetup_MLP :\n",
    "    torch.manual_seed(random_seed)\n",
    "    print('******************** Running MLP model (for PairSetup) ********************')\n",
    "    in_dim, out_dim = 14*14*2, 2\n",
    "    # We test for different hidden layer count vs. neurons per layer count\n",
    "    L_range = list(range(2, 17, 2))\n",
    "    h_range = list(range(4, 34, 4))\n",
    "    test_error_means = np.zeros((len(h_range), len(L_range)))\n",
    "    test_error_std = np.zeros((len(h_range), len(L_range)))\n",
    "    for L_idx in range(0, len(L_range)) :\n",
    "        for h_idx in range(0, len(h_range)) :\n",
    "    #for L_idx in [5] :\n",
    "    #    for h_idx in [1] :\n",
    "            torch.manual_seed(random_seed)\n",
    "            L = L_range[L_idx]\n",
    "            h = h_range[h_idx]\n",
    "            print('testing with L = {} and h = {}'.format(L, h))\n",
    "            model = models.MLP(L, h, in_dim, out_dim)\n",
    "            test_err_mean, test_err_std, _, _ = training_functions.rounds_train(model,\n",
    "                                                                                'PairSetup',\n",
    "                                                                                lr = 0.0001,\n",
    "                                                                                plot_title = 'Test',\n",
    "                                                                                plot_file_path='./plots/test.svg',\n",
    "                                                                                epochs = 300,\n",
    "                                                                                use_crossentropy=False)\n",
    "            test_error_means[h_idx, L_idx] = test_err_mean\n",
    "            test_error_std[h_idx, L_idx] = test_err_std\n",
    "    # Plot heat table\n",
    "    plots.plot_error_table(h_range, L_range, test_error_means, \n",
    "                 test_error_std, 'MLP (PairSetup) minimum test error mean/std',\n",
    "                          './plots/PairSetup_MLP.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **OLD (DRAFT)**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal : predicts if pair's 1st digit <= to the second (=0) or if pair's 1st digit > to the second (=1)\n",
    "\n",
    "Ideas of architectures to test :\n",
    "- Simple MLP (fully connected)\n",
    "- LetNet5\n",
    "- AlexNet\n",
    "- VGGNet19\n",
    "- Residual Net\n",
    "- Use cross entropy\n",
    "- Use dropout\n",
    "\n",
    "General framework to test :\n",
    "- Network is trained to predict directly lesser or greater\n",
    "- Network is trained to predict number, then we do the difference\n",
    "\n",
    "In this notebook we explore the first architecture only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(L, h)\n",
    "train_errors, test_errors = train(model, train_input, train_target, test_input, test_target, \n",
    "                                  use_crossentropy=False)\n",
    "plot_errors(train_errors, test_errors, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# MLP Models\n",
    "L_range = list(range(2, 17, 2))\n",
    "h_range = list(range(4, 34, 4))\n",
    "test_error_means = np.zeros((len(h_range), len(L_range)))\n",
    "test_error_std = np.zeros((len(h_range), len(L_range)))\n",
    "\n",
    "print('********** MLP models **********')\n",
    "for L_idx in range(0, len(L_range)) :\n",
    "    for h_idx in range(0, len(h_range)) :\n",
    "        L = L_range[L_idx]\n",
    "        h = h_range[h_idx]\n",
    "        print('testing with L = {} and h = {}'.format(L, h))\n",
    "        model = MLP(L, h)\n",
    "        test_err_mean, test_err_std, _, _ = multiple_rounds_train(model, use_crossentropy=False)\n",
    "        test_error_means[h_idx, L_idx] = test_err_mean\n",
    "        test_error_std[h_idx, L_idx] = test_err_std\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# LeNet5Like Model\n",
    "for dropout in [False, True] :\n",
    "    model = LeNet5Like(dropout=dropout)\n",
    "    train_errors, test_errors = train(model, train_input, train_target, test_input, test_target,\n",
    "                                     use_crossentropy = True)\n",
    "    plot_errors(train_errors, test_errors, 'LeNet5Like, DropOut = {}'.format(dropout))\n",
    "    del model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Custom \"LeNet3\" Model\n",
    "model = LeNet3()\n",
    "train_errors, test_errors = train(model, train_input, train_target, test_input, test_target,\n",
    "                                  use_crossentropy = True)\n",
    "plot_errors(train_errors, test_errors, 'LeNet3, {} hidden layers, {} neurons per layer'.format(L,h))\n",
    "del model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# VGGNetLike Model\n",
    "for dropout in [False, True] :\n",
    "    model = VGGNetLike(dropout=dropout)\n",
    "    train_errors, test_errors = train(model, train_input, train_target, test_input, test_target, \n",
    "                                     use_crossentropy = True)\n",
    "    plot_errors(train_errors, test_errors, 'VGGNetLike, DropOut = {}'.format(dropout))\n",
    "    del model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Custom \"ConvResNet\" Model\n",
    "model = ConvResNet()\n",
    "train_errors, test_errors = train(model, train_input, train_target, test_input, test_target,\n",
    "                                  use_crossentropy = True)\n",
    "plot_errors(train_errors, test_errors, 'ConvResNet, {} hidden layers, {} neurons per layer'.format(L,h))\n",
    "del model\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
