{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **NEW (future train.py)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import dlc_practical_prologue as prologue\n",
    "import numpy as np\n",
    "# Our modules\n",
    "import models # contains all our torch models classes\n",
    "import plots # custom ploting functions to produce figures of the report\n",
    "import training_functions # all our functions and classes for training\n",
    "\n",
    "\n",
    "# Set here which experiment is to be run (running them all will take a long time to compute)\n",
    "run_LossCompare = True\n",
    "# Experiments for \"PairSetup\" : In this setup, we consider directly the pairs as input to the \n",
    "# network. Thus our inputs are N samples of [2,14,14] made of two 14*14 pictures. Our ouputs are\n",
    "# the class 0 or 1  indicating whereas if the first digit is lesser or equal to the second.\n",
    "run_PairSetup_SimpleLinear = False\n",
    "run_PairSetup_MLP = False\n",
    "# Experiments for \"AuxiliarySetup\" : In this setup, we consider N individual 14*14 pictures as \n",
    "# input. The network use an auxiliary loss to learn to classify those from 0 to 9. The auxiliary\n",
    "# outputs are the the class 0 to 9 corresponding to the digit on the picture. We then use this \n",
    "# network to predict the number and we can then do the difference to perform our original goal\n",
    "# which is to predict whereas if the first digit is lesser or equal to the second\n",
    "run_AuxiliarySetup_SimpleLinear = False\n",
    "run_AuxiliarySetup_MLP = False\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "\n",
    "if run_LossCompare :\n",
    "    torch.manual_seed(random_seed)\n",
    "    print('******************** Running Loss comparaison ********************')\n",
    "    \n",
    "    print('--- PairSetup, MSE loss : ---')\n",
    "    in_dim, out_dim = 14*14*2, 2\n",
    "    model = models.MLP(4, 32, in_dim, out_dim)\n",
    "    test_err_mean, test_err_std, _, _ = training_functions.rounds_train(model, \n",
    "                                                                        'PairSetup',\n",
    "                                                                        plot_title = 'Test',\n",
    "                                                                        plot_file_path='./plots/test1.svg',\n",
    "                                                                        lr = 0.00008,\n",
    "                                                                        epochs = 150,\n",
    "                                                                        use_crossentropy=False,\n",
    "                                                                        rounds=10)\n",
    "    print('mean minimum test error : {0:.{1}f} %'.format(test_err_mean,1))\n",
    "    print('std minimum test error : {0:.{1}f} %'.format(test_err_std,1))\n",
    "    \n",
    "    print('--- PairSetup, cross-entropy loss : ---')\n",
    "    model = models.MLP(4, 32, in_dim, out_dim)\n",
    "    test_err_mean, test_err_std, _, _ = training_functions.rounds_train(model, \n",
    "                                                                        'PairSetup',\n",
    "                                                                        plot_title = 'Test',\n",
    "                                                                        plot_file_path='./plots/test2.svg',\n",
    "                                                                        lr = 0.00008,\n",
    "                                                                        epochs = 150,\n",
    "                                                                        use_crossentropy=True,\n",
    "                                                                        rounds=10)\n",
    "    print('mean minimum test error : {0:.{1}f} %'.format(test_err_mean,1))\n",
    "    print('std minimum test error : {0:.{1}f} %'.format(test_err_std,1))\n",
    "    \n",
    "    print('--- AuxiliarySetup, MSE loss : ---')\n",
    "    in_dim, out_dim = 14*14, 10\n",
    "    model = models.MLP(4, 32, in_dim, out_dim)\n",
    "    test_err_mean, test_err_std, _, _ = training_functions.rounds_train(model, \n",
    "                                                                        'AuxiliarySetup',\n",
    "                                                                        plot_title = 'Test',\n",
    "                                                                        plot_file_path='./plots/test1Aux.svg',\n",
    "                                                                        lr = 0.00008,\n",
    "                                                                        epochs = 300,\n",
    "                                                                        use_crossentropy=False,\n",
    "                                                                        rounds=10)\n",
    "    print('mean minimum test error : {0:.{1}f} %'.format(test_err_mean,1))\n",
    "    print('std minimum test error : {0:.{1}f} %'.format(test_err_std,1))\n",
    "    \n",
    "    print('--- AuxiliarySetup, cross-entropy loss : ---')\n",
    "    model = models.MLP(4, 32, in_dim, out_dim)\n",
    "    test_err_mean, test_err_std, _, _ = training_functions.rounds_train(model,\n",
    "                                                                        'AuxiliarySetup',\n",
    "                                                                        plot_title = 'Test',\n",
    "                                                                        plot_file_path='./plots/test2Aux.svg',\n",
    "                                                                        lr = 0.00008,\n",
    "                                                                        epochs = 150,\n",
    "                                                                        use_crossentropy=True,\n",
    "                                                                        rounds=10)\n",
    "    print('mean minimum test error : {0:.{1}f} %'.format(test_err_mean,1))\n",
    "    print('std minimum test error : {0:.{1}f} %'.format(test_err_std,1))\n",
    "    \n",
    "\n",
    "if run_PairSetup_SimpleLinear :\n",
    "    torch.manual_seed(random_seed)\n",
    "    print('******************** Running SimpleLinear model (for PairSetup) ********************')\n",
    "    model = models.SimpleLinear(in_dim, out_dim)\n",
    "    test_err_mean, test_err_std, _, _ = rounds_train(model,\n",
    "                                                     plot_title = 'Linear (PairSetup) Train & Test errors',\n",
    "                                                     plot_file_path='./plots/pairSetup_SimpleLinear.svg',\n",
    "                                                     verbose=True,\n",
    "                                                     lr = 0.0001,\n",
    "                                                     epochs = 150,\n",
    "                                                     use_crossentropy=False,\n",
    "                                                     rounds=1)\n",
    "    print('mean minimum test error : {0:.{1}f} %'.format(test_err_mean,1))\n",
    "    print('std minimum test error : {0:.{1}f} %'.format(test_err_std,1))\n",
    "\n",
    "\n",
    "if run_PairSetup_MLP :\n",
    "    torch.manual_seed(random_seed)\n",
    "    print('******************** Running MLP model (for PairSetup) ********************')\n",
    "    # We test for different hidden layer count vs. neurons per layer count\n",
    "    L_range = list(range(2, 17, 2))\n",
    "    h_range = list(range(4, 34, 4))\n",
    "    test_error_means = np.zeros((len(h_range), len(L_range)))\n",
    "    test_error_std = np.zeros((len(h_range), len(L_range)))\n",
    "    for L_idx in range(0, len(L_range)) :\n",
    "        for h_idx in range(0, len(h_range)) :\n",
    "    #for L_idx in [5] :\n",
    "    #    for h_idx in [1] :\n",
    "            torch.manual_seed(random_seed)\n",
    "            L = L_range[L_idx]\n",
    "            h = h_range[h_idx]\n",
    "            print('testing with L = {} and h = {}'.format(L, h))\n",
    "            model = models.MLP(L, h, in_dim, out_dim)\n",
    "            print('hey')\n",
    "            test_err_mean, test_err_std, _, _ = rounds_train(model,\n",
    "                                                             verbose=True,\n",
    "                                                             lr = 0.0001,\n",
    "                                                             plot_title = 'Test',\n",
    "                                                             plot_file_path='./plots/test.svg',\n",
    "                                                             use_crossentropy=False)\n",
    "            test_error_means[h_idx, L_idx] = test_err_mean\n",
    "            test_error_std[h_idx, L_idx] = test_err_std\n",
    "    # Plot heat table\n",
    "    plots.plot_error_table(h_range, L_range, test_error_means, \n",
    "                 test_error_std, 'MLP (PairSetup) minimum test error mean/std',\n",
    "                          './plots/PairSetup_MLP.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **OLD (DRAFT)**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal : predicts if pair's 1st digit <= to the second (=0) or if pair's 1st digit > to the second (=1)\n",
    "\n",
    "Ideas of architectures to test :\n",
    "- Simple MLP (fully connected)\n",
    "- LetNet5\n",
    "- AlexNet\n",
    "- VGGNet19\n",
    "- Residual Net\n",
    "- Use cross entropy\n",
    "- Use dropout\n",
    "\n",
    "General framework to test :\n",
    "- Network is trained to predict directly lesser or greater\n",
    "- Network is trained to predict number, then we do the difference\n",
    "\n",
    "In this notebook we explore the first architecture only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, train_input, train_target, augment_data=False):\n",
    "        if augment_data :\n",
    "            # TODO : Full data augmentation instead (randomized pairs)\n",
    "            # Create the inversed pairs (data augmentation)\n",
    "            train_input_rev = train_input[:,[1,0],:,:]\n",
    "            train_target_rev = train_target[:,[1,0]]\n",
    "            self.train_input = torch.cat((train_input,train_input_rev))\n",
    "            self.train_target = torch.cat((train_target,train_target_rev))\n",
    "        else :\n",
    "            self.train_input = train_input\n",
    "            self.train_target = train_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'pair': self.train_input[idx], 'target': self.train_target[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_encode(data):\n",
    "    \"\"\" 2-class hot encoding of target \"\"\"\n",
    "    col_view = data.view(-1,1)\n",
    "    return torch.cat((col_view == 0, col_view == 1), dim=1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_deencode(data):\n",
    "    \"\"\" from hot-encoding back to class labels \"\"\"\n",
    "    return torch.argmax(data, dim=1).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errors(output, target):\n",
    "    \"\"\" Computes error percentage given output and target\"\"\"\n",
    "    errors_amount = (output.argmax(dim=1) != target.argmax(dim=1)).sum().item()\n",
    "    return (errors_amount / output.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_input, train_target, test_input, test_target, \n",
    "          use_crossentropy = False, lr=1e-3, epochs = 200, verbose=False) :\n",
    "    \n",
    "    batch_size = 200\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "    criterion = nn.CrossEntropyLoss() if use_crossentropy else nn.MSELoss()\n",
    "    \n",
    "    pairsDataset = PairsDataset(train_input, train_target)\n",
    "    dataloader = DataLoader(pairsDataset, batch_size=batch_size, shuffle=True, \n",
    "                            num_workers=0)\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        if verbose :\n",
    "            print('.', end='')\n",
    "        for i_batch, batch in enumerate(dataloader):\n",
    "            inputPairs = batch['pair']\n",
    "            # If we use crossentropy, then we don't want hot-encoding of target class but \n",
    "            # directly their class.\n",
    "            target = hot_deencode(batch['target']) if use_crossentropy else batch['target']\n",
    "            # Forward pass\n",
    "            output = model(inputPairs)\n",
    "            # Compute loss\n",
    "            loss = criterion(output, target)\n",
    "            # Backprop & update parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Compute train error\n",
    "            output_train = model(train_input)\n",
    "            train_errors.append(compute_errors(output_train, train_target))\n",
    "            # Compute test error\n",
    "            output_test = model(test_input)\n",
    "            test_errors.append(compute_errors(output_test, test_target))\n",
    "                \n",
    "    return train_errors, test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors(train_errors, test_errors, title):\n",
    "    plt.title(title)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('error %')\n",
    "    plt.plot(train_errors, label='train error %')\n",
    "    plt.plot(test_errors, label='test error %')\n",
    "    plt.ylim(0, 60)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    print('Last train error : {}'.format(test_errors[-1]))\n",
    "    print('Smallest train error : {}'.format(min(test_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_rounds_train(model, rounds=10, N=1000, augment_data=False, use_crossentropy=True, \n",
    "                          lr=1e-3, epochs=200, verbose=False):\n",
    "    \n",
    "    min_test_errors = []\n",
    "    corresponding_train_errors = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(0, rounds):\n",
    "        if verbose :\n",
    "            print('round {}'.format(i+1))\n",
    "        # Load new data (data is randomized at each round as required in the project instructions)\n",
    "        train_input, train_target, _ , test_input, test_target, _ = prologue.generate_pair_sets(N)\n",
    "        # Hot encoding (target will be de-encoded if the loss is cross-entropy)\n",
    "        train_target = hot_encode(train_target)\n",
    "        test_target = hot_encode(test_target)\n",
    "        # Create dataset (let use use shuffled batch, but still reproducible thanks to the manual seed)\n",
    "        pairsDataset = PairsDataset(train_input, train_target, augment_data=augment_data)\n",
    "        # Train\n",
    "        train_errors, test_errors = train(model, train_input, train_target, test_input, test_target, \n",
    "          use_crossentropy = use_crossentropy, lr=lr, epochs=epochs)\n",
    "        # Store those histories\n",
    "        train_errors_histories.append(train_errors)\n",
    "        test_errors_histories.append(test_errors)\n",
    "        # Use use early stopping, we take the train & test error where the test error was the smallest\n",
    "        min_test_errors.append(min(test_errors))\n",
    "        corresponding_train_errors.append(train_errors[test_errors.index(min(test_errors))])\n",
    "    \n",
    "    # Compute and return mean/std of min test error and its corresponding train error    \n",
    "    return (np.mean(min_test_errors), np.std(min_test_errors), \n",
    "            np.mean(corresponding_train_errors), np.std(corresponding_train_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000\n",
    "# Load new data (data is randomized at each round as required in the project instructions)\n",
    "train_input, train_target, _ , test_input, test_target, _ = prologue.generate_pair_sets(N)\n",
    "# Hot encoding (target will be de-encoded if the loss is cross-entropy)\n",
    "train_target = hot_encode(train_target)\n",
    "test_target = hot_encode(test_target)\n",
    "# Create dataset (let use use shuffled batch, but still reproducible thanks to the manual seed)\n",
    "pairsDataset = PairsDataset(train_input, train_target, augment_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(L, h)\n",
    "train_errors, test_errors = train(model, train_input, train_target, test_input, test_target, \n",
    "                                  use_crossentropy=False)\n",
    "plot_errors(train_errors, test_errors, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Models\n",
    "L_range = list(range(2, 17, 2))\n",
    "h_range = list(range(4, 34, 4))\n",
    "test_error_means = np.zeros((len(h_range), len(L_range)))\n",
    "test_error_std = np.zeros((len(h_range), len(L_range)))\n",
    "\n",
    "print('********** MLP models **********')\n",
    "for L_idx in range(0, len(L_range)) :\n",
    "    for h_idx in range(0, len(h_range)) :\n",
    "        L = L_range[L_idx]\n",
    "        h = h_range[h_idx]\n",
    "        print('testing with L = {} and h = {}'.format(L, h))\n",
    "        model = MLP(L, h)\n",
    "        test_err_mean, test_err_std, _, _ = multiple_rounds_train(model, use_crossentropy=False)\n",
    "        test_error_means[h_idx, L_idx] = test_err_mean\n",
    "        test_error_std[h_idx, L_idx] = test_err_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_table(h_range, L_range, table_mean, table_std, title):\n",
    "    \n",
    "    h_labels = ['h = {}'.format(h) for h in h_range]\n",
    "    L_labels = ['L = {}'.format(L) for L in L_range]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    im = ax.imshow(table_mean)\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(L_labels)))\n",
    "    ax.set_yticks(np.arange(len(h_labels)))\n",
    "    ax.set_xticklabels(L_labels)\n",
    "    ax.set_yticklabels(h_labels)\n",
    "    \n",
    "    # Rotate the tick labels and set their alignment\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right',\n",
    "             rotation_mode='anchor')\n",
    "    \n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(h_labels)):\n",
    "        for j in range(len(L_labels)):\n",
    "            text = ax.text(j, i, '{0:.{1}f}'.format(table_mean[i, j], 1),\n",
    "                           ha='center', va='bottom', color='w')\n",
    "            text = ax.text(j, i, '({0:.{1}f})'.format(table_std[i, j], 1),\n",
    "                           ha='center', va='top', color='#FFFFFF80')\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_error_table(h_range, L_range, test_error_means, \n",
    "                 test_error_std, 'MLP Min-Error % Mean & std for L hidden layers with h neurons each')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset seed between each experiment so that we can run experiments in any orders\n",
    "# without impacting reporducibility\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet5Like Model\n",
    "for dropout in [False, True] :\n",
    "    model = LeNet5Like(dropout=dropout)\n",
    "    train_errors, test_errors = train(model, train_input, train_target, test_input, test_target,\n",
    "                                     use_crossentropy = True)\n",
    "    plot_errors(train_errors, test_errors, 'LeNet5Like, DropOut = {}'.format(dropout))\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom \"LeNet3\" Model\n",
    "model = LeNet3()\n",
    "train_errors, test_errors = train(model, train_input, train_target, test_input, test_target,\n",
    "                                  use_crossentropy = True)\n",
    "plot_errors(train_errors, test_errors, 'LeNet3, {} hidden layers, {} neurons per layer'.format(L,h))\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGGNetLike Model\n",
    "for dropout in [False, True] :\n",
    "    model = VGGNetLike(dropout=dropout)\n",
    "    train_errors, test_errors = train(model, train_input, train_target, test_input, test_target, \n",
    "                                     use_crossentropy = True)\n",
    "    plot_errors(train_errors, test_errors, 'VGGNetLike, DropOut = {}'.format(dropout))\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom \"ConvResNet\" Model\n",
    "model = ConvResNet()\n",
    "train_errors, test_errors = train(model, train_input, train_target, test_input, test_target,\n",
    "                                  use_crossentropy = True)\n",
    "plot_errors(train_errors, test_errors, 'ConvResNet, {} hidden layers, {} neurons per layer'.format(L,h))\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
