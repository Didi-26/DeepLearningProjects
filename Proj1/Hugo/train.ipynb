{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **NEW (future train.py)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import dlc_practical_prologue as prologue\n",
    "import numpy as np\n",
    "# Our modules\n",
    "import models # contains all our torch models classes\n",
    "import plots # custom plotting functions to produce figures of the report\n",
    "\n",
    "\n",
    "# Set here which experiment is to be run (running them all will take a long time to compute)\n",
    "\n",
    "# Experiments for \"PairSetup\" : In this setup, we consider directly the pairs as input to the \n",
    "# network. Thus our inputs are N samples of [2,14,14] made of two 14*14 pictures. Our ouputs are\n",
    "# the class 0 or 1  indicating whereas if the first digit is lesser or equal to the second.\n",
    "run_PairSetup_SimpleLinear = True\n",
    "run_PairSetup_MLP = True\n",
    "#...\n",
    "\n",
    "# Experiments for \"AuxiliarySetup\" : In this setup, we consider N individual 14*14 pictures as \n",
    "# input. The network use an auxiliary loss to learn to classify those from 0 to 9. The auxiliary\n",
    "# outputs are the the class 0 to 9 corresponding to the digit on the picture. We then use this \n",
    "# network to predict the number and we can then do the difference to perform our original goal\n",
    "# which is to predict whereas if the first digit is lesser or equal to the second\n",
    "run_AuxiliarySetup_SimpleLinear = True\n",
    "run_AuxiliarySetup_MLP = True\n",
    "#...\n",
    "\n",
    "class TrainPairsDataset(Dataset):\n",
    "    \"\"\" \n",
    "    PyTorch Dataset for holding MNIST train pairs. \n",
    "    Arguments: \n",
    "        - train_input: a torch tensor of size [N, 2, 14, 14] containing the N training pairs\n",
    "        - train_target: hot-encoded target torch tensor of size [N , 2]\n",
    "        - augment_data: boolean, if True, data will be augmented by inversing pairs\n",
    "    \"\"\"\n",
    "    def __init__(self, train_input, train_target, augment_data=False):\n",
    "        if augment_data :\n",
    "            # Create the inversed pairs (data augmentation)\n",
    "            train_input_rev = train_input[:,[1,0],:,:]\n",
    "            train_target_rev = train_target[:,[1,0]]\n",
    "            self.train_input = torch.cat((train_input,train_input_rev))\n",
    "            self.train_target = torch.cat((train_target,train_target_rev))\n",
    "        else :\n",
    "            self.train_input = train_input\n",
    "            self.train_target = train_target\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.train_input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'pair': self.train_input[idx], 'target': self.train_target[idx]}\n",
    "\n",
    "    \n",
    "def hot_encode(data):\n",
    "    \"\"\" \n",
    "    2-class hot encoding.\n",
    "    Arguments:\n",
    "        - data: torch tensor of size N of 0s and 1s\n",
    "    Returns:\n",
    "        - hot-encoded torch tensor of size [N,2]\n",
    "    \"\"\"\n",
    "    col_view = data.view(-1,1)\n",
    "    return torch.cat((col_view == 0, col_view == 1), dim=1).float()\n",
    "\n",
    "\n",
    "def hot_decode(data):\n",
    "    \"\"\"\n",
    "    2-class hot decoding. Performs the inverse of the function hot_encode().\n",
    "    Arguments:\n",
    "        - data: hot-encoded torch tensor of size [N,2]\n",
    "    Returns:\n",
    "        - hot-decoded torch tensor of size N of 0s and 1s\n",
    "    \"\"\"\n",
    "    return torch.argmax(data, dim=1).long()\n",
    "\n",
    "\n",
    "def compute_errors(output, target):\n",
    "    \"\"\" \n",
    "    Computes error percentage given output and target\n",
    "    Arguments:\n",
    "        - output: torch tensor of [N,2] of predicted scores for each class\n",
    "        - target: hot-encoded target torch tensor of size [N,2]\n",
    "    Returns:\n",
    "        - error %\n",
    "    \"\"\"\n",
    "    errors_amount = (output.argmax(dim=1) != target.argmax(dim=1)).sum().item()\n",
    "    return (errors_amount / output.shape[0]) * 100\n",
    "\n",
    "\n",
    "def train(model, train_input, train_target, test_input, test_target, \n",
    "          use_crossentropy = False, lr=1e-3, epochs = 200, verbose=False) :\n",
    "    \"\"\" \n",
    "    Trains the given model using the given train and test dataset. Returns the\n",
    "    train & test error % history.\n",
    "    Arguments:\n",
    "        - model: torch model to train\n",
    "        - train_input: torch tensor of train input data\n",
    "        - train_target: hot-encoded train target class\n",
    "        - test_input: torch tensor of test input data\n",
    "        - test_target: hot-encoded test target class\n",
    "        - use_crossentropy: boolean, if True, crossentropy loss will be used (train \n",
    "        target data will be dencoded in order to use this loss). If False MSE loss \n",
    "        will be used.\n",
    "        - lr: learning rate\n",
    "        - epochs: number of epochs to train with\n",
    "        - verbose: if True, a dot '.' will be printed at each new epoch\n",
    "    Returns:\n",
    "        - (train_errors, test_errors), the train and test error % histories\n",
    "    \"\"\" \n",
    "    batch_size = 100\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "    criterion = nn.CrossEntropyLoss() if use_crossentropy else nn.MSELoss()\n",
    "    \n",
    "    trainPairsDataset = TrainPairsDataset(train_input, train_target)\n",
    "    dataloader = DataLoader(trainPairsDataset, batch_size=batch_size, shuffle=True, \n",
    "                            num_workers=0)\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        if verbose :\n",
    "            print('.', end='')\n",
    "        for i_batch, batch in enumerate(dataloader):\n",
    "            inputPairs = batch['pair']\n",
    "            # If we use crossentropy, then we don't want hot-encoding of train target class but \n",
    "            # directly their class.\n",
    "            target = hot_decode(batch['target']) if use_crossentropy else batch['target']\n",
    "            # Forward pass\n",
    "            output = model(inputPairs)\n",
    "            # Compute loss\n",
    "            loss = criterion(output, target)\n",
    "            # Backprop & update parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Compute train error\n",
    "            output_train = model(train_input)\n",
    "            train_errors.append(compute_errors(output_train, train_target))\n",
    "            # Compute test error\n",
    "            output_test = model(test_input)\n",
    "            test_errors.append(compute_errors(output_test, test_target))\n",
    "                \n",
    "    return train_errors, test_errors\n",
    "\n",
    "\n",
    "def rounds_train(model, rounds=10, augment_data=False, use_crossentropy=True, \n",
    "                          lr=1e-3, epochs=200, verbose=False, plot_title = None,\n",
    "                          plot_file_path = None):\n",
    "    \"\"\"\n",
    "    Trains the model multiple times with randomized fresh new data. For each training, we keep\n",
    "    only the best test error (early stopping) with its corresponding train error. Then we compute\n",
    "    and return the average and standard deviation of those best test errors and their corresponding \n",
    "    train errors.\n",
    "    Arguments:\n",
    "        - model: torch model to train\n",
    "        - rounds: number of experiment (new training) to perform\n",
    "        - augment_data : boolean, if True, data will be augmented by inversing pairs\n",
    "        - use_crossentropy: boolean, if True, crossentropy loss will be used. If False MSE loss will \n",
    "        be used.\n",
    "        - lr: learning rate\n",
    "        - epochs: number of epochs to train with at each round\n",
    "        - verbose: if True, the current round will be printed at each round\n",
    "        - plot_title : str, if plot_title and plot_file_path are not None, then a figure with \n",
    "        the given titlt will be saved at the given path\n",
    "        - plot_file_path : str, if plot_title and plot_file_path are not None, then a figure with \n",
    "        the given titlt will be saved at the given path\n",
    "    Returns:\n",
    "        - (min_test_mean, min_test_std, min_tran_mean, min_train_std), the average and standard deviation\n",
    "        of the min-train (and corresponding test) errors %.\n",
    "    \"\"\"\n",
    "    min_test_errors = []\n",
    "    corresponding_train_errors = []\n",
    "    train_errors_histories = []\n",
    "    test_errors_histories = []\n",
    "    \n",
    "    # Number of samples to generate each round (=1000 as per the project instructions)\n",
    "    N=1000  \n",
    "    \n",
    "    for i in range(0, rounds):\n",
    "        if verbose :\n",
    "            print('round nÂ°{}'.format(i+1))\n",
    "        # Load new data (data is randomized at each round as required in the project instructions)\n",
    "        train_input, train_target, _ , test_input, test_target, _ = prologue.generate_pair_sets(N)\n",
    "        # Hot encoding (train targets will be later dencoded if the loss is cross-entropy)\n",
    "        train_target = hot_encode(train_target)\n",
    "        test_target = hot_encode(test_target)\n",
    "        # Create dataset (let use use shuffled batch, but still reproducible thanks to the manual seed)\n",
    "        pairsDataset = TrainPairsDataset(train_input, train_target, augment_data=augment_data)\n",
    "        # Train\n",
    "        train_errors, test_errors = train(model, train_input, train_target, test_input, test_target, \n",
    "          use_crossentropy = use_crossentropy, lr=lr, epochs=epochs)\n",
    "        # Store those histories\n",
    "        train_errors_histories.append(train_errors)\n",
    "        test_errors_histories.append(test_errors)\n",
    "        # Use use early stopping, we take the train & test error where the test error was the smallest\n",
    "        min_test_errors.append(min(test_errors))\n",
    "        corresponding_train_errors.append(train_errors[test_errors.index(min(test_errors))])\n",
    "    \n",
    "    # Plot figure\n",
    "    if (plot_title != None) and (plot_file_path != None):\n",
    "        plots.plot_errors(train_errors_histories, test_errors_histories, plot_title, plot_file_path)\n",
    "    \n",
    "    # Compute and return mean/std of min test error and its corresponding train error    \n",
    "    return (np.mean(min_test_errors), np.std(min_test_errors), \n",
    "            np.mean(corresponding_train_errors), np.std(corresponding_train_errors))\n",
    "\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "if run_PairSetup_SimpleLinear :\n",
    "    print('******************** Running SimpleLinear model (for PairSetup) ********************')\n",
    "    torch.manual_seed(random_seed)\n",
    "    in_dim = 14*14*2\n",
    "    out_dim = 2\n",
    "    model = models.SimpleLinear(in_dim, out_dim)\n",
    "    test_err_mean, test_err_std, _, _ = rounds_train(model,\n",
    "                                                     plot_title = 'Linear Train & Test errors',\n",
    "                                                     plot_file_path='./plots/pairSetup_SimpleLinear.eps',\n",
    "                                                     verbose=True)\n",
    "    print('done')\n",
    "\n",
    "if run_PairSetup_MLP :\n",
    "    print('******************** Running MLP model (for PairSetup) ********************')\n",
    "    torch.manual_seed(random_seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **OLD (DRAFT)**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal : predicts if pair's 1st digit <= to the second (=0) or if pair's 1st digit > to the second (=1)\n",
    "\n",
    "Ideas of architectures to test :\n",
    "- Simple MLP (fully connected)\n",
    "- LetNet5\n",
    "- AlexNet\n",
    "- VGGNet19\n",
    "- Residual Net\n",
    "- Use cross entropy\n",
    "- Use dropout\n",
    "\n",
    "General framework to test :\n",
    "- Network is trained to predict directly lesser or greater\n",
    "- Network is trained to predict number, then we do the difference\n",
    "\n",
    "In this notebook we explore the first architecture only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, train_input, train_target, augment_data=False):\n",
    "        if augment_data :\n",
    "            # TODO : Full data augmentation instead (randomized pairs)\n",
    "            # Create the inversed pairs (data augmentation)\n",
    "            train_input_rev = train_input[:,[1,0],:,:]\n",
    "            train_target_rev = train_target[:,[1,0]]\n",
    "            self.train_input = torch.cat((train_input,train_input_rev))\n",
    "            self.train_target = torch.cat((train_target,train_target_rev))\n",
    "        else :\n",
    "            self.train_input = train_input\n",
    "            self.train_target = train_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'pair': self.train_input[idx], 'target': self.train_target[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_encode(data):\n",
    "    \"\"\" 2-class hot encoding of target \"\"\"\n",
    "    col_view = data.view(-1,1)\n",
    "    return torch.cat((col_view == 0, col_view == 1), dim=1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_deencode(data):\n",
    "    \"\"\" from hot-encoding back to class labels \"\"\"\n",
    "    return torch.argmax(data, dim=1).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errors(output, target):\n",
    "    \"\"\" Computes error percentage given output and target\"\"\"\n",
    "    errors_amount = (output.argmax(dim=1) != target.argmax(dim=1)).sum().item()\n",
    "    return (errors_amount / output.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_input, train_target, test_input, test_target, \n",
    "          use_crossentropy = False, lr=1e-3, epochs = 200, verbose=False) :\n",
    "    \n",
    "    batch_size = 200\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "    criterion = nn.CrossEntropyLoss() if use_crossentropy else nn.MSELoss()\n",
    "    \n",
    "    pairsDataset = PairsDataset(train_input, train_target)\n",
    "    dataloader = DataLoader(pairsDataset, batch_size=batch_size, shuffle=True, \n",
    "                            num_workers=0)\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        if verbose :\n",
    "            print('.', end='')\n",
    "        for i_batch, batch in enumerate(dataloader):\n",
    "            inputPairs = batch['pair']\n",
    "            # If we use crossentropy, then we don't want hot-encoding of target class but \n",
    "            # directly their class.\n",
    "            target = hot_deencode(batch['target']) if use_crossentropy else batch['target']\n",
    "            # Forward pass\n",
    "            output = model(inputPairs)\n",
    "            # Compute loss\n",
    "            loss = criterion(output, target)\n",
    "            # Backprop & update parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Compute train error\n",
    "            output_train = model(train_input)\n",
    "            train_errors.append(compute_errors(output_train, train_target))\n",
    "            # Compute test error\n",
    "            output_test = model(test_input)\n",
    "            test_errors.append(compute_errors(output_test, test_target))\n",
    "                \n",
    "    return train_errors, test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors(train_errors, test_errors, title):\n",
    "    plt.title(title)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('error %')\n",
    "    plt.plot(train_errors, label='train error %')\n",
    "    plt.plot(test_errors, label='test error %')\n",
    "    plt.ylim(0, 60)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    print('Last train error : {}'.format(test_errors[-1]))\n",
    "    print('Smallest train error : {}'.format(min(test_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_rounds_train(model, rounds=10, N=1000, augment_data=False, use_crossentropy=True, \n",
    "                          lr=1e-3, epochs=200, verbose=False):\n",
    "    \n",
    "    min_test_errors = []\n",
    "    corresponding_train_errors = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(0, rounds):\n",
    "        if verbose :\n",
    "            print('round {}'.format(i+1))\n",
    "        # Load new data (data is randomized at each round as required in the project instructions)\n",
    "        train_input, train_target, _ , test_input, test_target, _ = prologue.generate_pair_sets(N)\n",
    "        # Hot encoding (target will be de-encoded if the loss is cross-entropy)\n",
    "        train_target = hot_encode(train_target)\n",
    "        test_target = hot_encode(test_target)\n",
    "        # Create dataset (let use use shuffled batch, but still reproducible thanks to the manual seed)\n",
    "        pairsDataset = PairsDataset(train_input, train_target, augment_data=augment_data)\n",
    "        # Train\n",
    "        train_errors, test_errors = train(model, train_input, train_target, test_input, test_target, \n",
    "          use_crossentropy = use_crossentropy, lr=lr, epochs=epochs)\n",
    "        # Store those histories\n",
    "        train_errors_histories.append(train_errors)\n",
    "        test_errors_histories.append(test_errors)\n",
    "        # Use use early stopping, we take the train & test error where the test error was the smallest\n",
    "        min_test_errors.append(min(test_errors))\n",
    "        corresponding_train_errors.append(train_errors[test_errors.index(min(test_errors))])\n",
    "    \n",
    "    # Compute and return mean/std of min test error and its corresponding train error    \n",
    "    return (np.mean(min_test_errors), np.std(min_test_errors), \n",
    "            np.mean(corresponding_train_errors), np.std(corresponding_train_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000\n",
    "# Load new data (data is randomized at each round as required in the project instructions)\n",
    "train_input, train_target, _ , test_input, test_target, _ = prologue.generate_pair_sets(N)\n",
    "# Hot encoding (target will be de-encoded if the loss is cross-entropy)\n",
    "train_target = hot_encode(train_target)\n",
    "test_target = hot_encode(test_target)\n",
    "# Create dataset (let use use shuffled batch, but still reproducible thanks to the manual seed)\n",
    "pairsDataset = PairsDataset(train_input, train_target, augment_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(L, h)\n",
    "train_errors, test_errors = train(model, train_input, train_target, test_input, test_target, \n",
    "                                  use_crossentropy=False)\n",
    "plot_errors(train_errors, test_errors, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Models\n",
    "L_range = list(range(2, 17, 2))\n",
    "h_range = list(range(4, 34, 4))\n",
    "test_error_means = np.zeros((len(h_range), len(L_range)))\n",
    "test_error_std = np.zeros((len(h_range), len(L_range)))\n",
    "\n",
    "print('********** MLP models **********')\n",
    "for L_idx in range(0, len(L_range)) :\n",
    "    for h_idx in range(0, len(h_range)) :\n",
    "        L = L_range[L_idx]\n",
    "        h = h_range[h_idx]\n",
    "        print('testing with L = {} and h = {}'.format(L, h))\n",
    "        model = MLP(L, h)\n",
    "        test_err_mean, test_err_std, _, _ = multiple_rounds_train(model, use_crossentropy=False)\n",
    "        test_error_means[h_idx, L_idx] = test_err_mean\n",
    "        test_error_std[h_idx, L_idx] = test_err_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_table(h_range, L_range, table_mean, table_std, title):\n",
    "    \n",
    "    h_labels = ['h = {}'.format(h) for h in h_range]\n",
    "    L_labels = ['L = {}'.format(L) for L in L_range]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    im = ax.imshow(table_mean)\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(L_labels)))\n",
    "    ax.set_yticks(np.arange(len(h_labels)))\n",
    "    ax.set_xticklabels(L_labels)\n",
    "    ax.set_yticklabels(h_labels)\n",
    "    \n",
    "    # Rotate the tick labels and set their alignment\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right',\n",
    "             rotation_mode='anchor')\n",
    "    \n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(h_labels)):\n",
    "        for j in range(len(L_labels)):\n",
    "            text = ax.text(j, i, '{0:.{1}f}'.format(table_mean[i, j], 1),\n",
    "                           ha='center', va='bottom', color='w')\n",
    "            text = ax.text(j, i, '({0:.{1}f})'.format(table_std[i, j], 1),\n",
    "                           ha='center', va='top', color='#FFFFFF80')\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_error_table(h_range, L_range, test_error_means, \n",
    "                 test_error_std, 'MLP Min-Error % Mean & std for L hidden layers with h neurons each')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset seed between each experiment so that we can run experiments in any orders\n",
    "# without impacting reporducibility\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet5Like Model\n",
    "for dropout in [False, True] :\n",
    "    model = LeNet5Like(dropout=dropout)\n",
    "    train_errors, test_errors = train(model, train_input, train_target, test_input, test_target,\n",
    "                                     use_crossentropy = True)\n",
    "    plot_errors(train_errors, test_errors, 'LeNet5Like, DropOut = {}'.format(dropout))\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom \"LeNet3\" Model\n",
    "model = LeNet3()\n",
    "train_errors, test_errors = train(model, train_input, train_target, test_input, test_target,\n",
    "                                  use_crossentropy = True)\n",
    "plot_errors(train_errors, test_errors, 'LeNet3, {} hidden layers, {} neurons per layer'.format(L,h))\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGGNetLike Model\n",
    "for dropout in [False, True] :\n",
    "    model = VGGNetLike(dropout=dropout)\n",
    "    train_errors, test_errors = train(model, train_input, train_target, test_input, test_target, \n",
    "                                     use_crossentropy = True)\n",
    "    plot_errors(train_errors, test_errors, 'VGGNetLike, DropOut = {}'.format(dropout))\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom \"ConvResNet\" Model\n",
    "model = ConvResNet()\n",
    "train_errors, test_errors = train(model, train_input, train_target, test_input, test_target,\n",
    "                                  use_crossentropy = True)\n",
    "plot_errors(train_errors, test_errors, 'ConvResNet, {} hidden layers, {} neurons per layer'.format(L,h))\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
